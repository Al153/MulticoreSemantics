\documentclass[11pt]{article}
\newcommand\comment[1]{}
\usepackage{graphicx}
\usepackage{a4wide,parskip,times}


\newcommand{\todo}[1]{\textbf{#1}}
\newcommand{\deno}[1]{{\bf [\![}#1{\bf ]\!]}}



\newcommand{\st}{$^{st}$}
\renewcommand{\th}{$^{th}$}
\newcommand{\nd}{$^{nd}$}
\newcommand{\rd}{$^{rd}$}

\begin{document}

\centerline{\Large Multicore Semantics and Programming}
\vspace{2em}
\centerline{\Large \emph{Practical Report}}
\vspace{2em}
\centerline{\large A. J. Taylor (\emph{at736}), St John's College}
\vspace{1em}

\begin{abstract}
\textsl{
	A written report for Tim Harris' section of the course
} 
\end{abstract}


\todo{Fix figure labels}

\section{Summary of Experimental Conditions and Methods}

\subsection{Hardware}
Experiments were carried out on a HP Spectre Laptop, which was plugged in and on maximum performance settings. The laptop has a quad core, hyperthreaded, intel i7 8550u processor for a total of 8 physical threads (two threads per core) \footnote{https://ark.intel.com/products/122589/Intel-Core-i7-8550U-Processor-8M-Cache-up-to-4-00-GHz-}.

\subsection{Experimental Methods}
Experiments were written in Java and run under Windows 10. The laptop was set not to sleep for the duration of each experiment and other user processes were kept to a minimum to improve reliability of the results. The construction of all objects used by the performance tests is kept in the setup code and is not timed. Furthermore, GC runs are forced between performance tests and no individual test is expected to produce enough objects to make a GC call mid-test-run.

\subsection{Code Written}
The code used to run experiments and process the resulting data can be found on a dedicated Github repository\footnote{https://github.com/Al153/MulticoreSemantics}. I created an abstract \texttt{SharedArray} class containing an array with specifiable length and an abstract \texttt{sum} (read) and \texttt{update} (write) operations. Appropriate subclasses of this class were created with the \texttt{sum} and \texttt{update} operations taking the correct locks.


\section{The Experiments}
\subsection{Set Up and Initial Test}
The supplied test code ran correctly.
\subsection{Simple Multithreading}
\begin{figure}\label{fig:step2_1}
\centering
\includegraphics[scale=0.65]{step2.png}
 \caption{Time to complete for each thread running. Error bars, as is the case in the rest of this report, represent a single standard deviation on either side.}
\end{figure}

In this experiment, (Fig \ref{fig:step2_1}), repeated 100 times per number of threads, performance stays roughly equal for $n = 1, 2$, then begins to increase monotonically. As might be expected, there is a larger increase between $n= 4k$ and $n= 4k +1$ than in other intervals of $n$. This occurs because $4k$ operations can be scheduled within $k$ time units, whereas in the best case, $4k+1$ requires $k+1$ time units. In an ideal system, we might see the line be flat for $n = 4k+1 4k+2, 4k+3, 4(k+1)$, as each core would be utilised fully. We also see a deterioration in performance between one thread per core (1, 2, 3, 4) cores and two threads per core (5, 6, 7, 8) as the delay operation between threads are identical, tight, loops with little unpredictable memory access, meaning that simultaneous multithreading (hyperthreading) is unable to find many redundant pipeline stages to exploit.


\subsection{Read Only Shared-Arrays}
\paragraph{Unsafe Shared Array}

\begin{figure}\label{fig:step3_1}
\centering
\includegraphics[scale=0.65]{step3_1.png}
\caption{Speed of the unsafe shared array when instantiated with various sizes}
\end{figure}


\begin{figure}\label{fig:step3_2}
\centering
\includegraphics[scale=0.65]{step3_2.png}
\caption{Speed of the safe, mutex locked, shared array when instantiated with various sizes}
\end{figure}

The locked array (Fig \ref{fig:step3_2}) did not perform significantly worse in the average case than the unsafe array (Fig \ref{fig:step3_1}),
however the variance increases significantly as the number of threads and size of the array increases. I expect this is due to a one sided distribution, where the majority of results are scattered close to the mean with some large outliers which took much longer due to a delay in loading into local cache and taking a lock. The difference in standard deviation and means for the largest array size is shown in Fig \ref{fig:step3_3}. The fact that the mean time to complete does not increase substantially with increasing thread count suggests that the bottleneck for each loop in the common case is not taking the lock or running the sum operation but instead due to overheads like loading the array to cache to be summed and checking exit conditions for the loop.


\begin{figure}\label{fig:step3_3}
\centering
\includegraphics[scale=0.65]{step3_3.png}
\caption{Comparative speed of the unsafe and mutex-locked arrays set to a size of $X=5000$. The standard deviation is much more constant for the unsafe version.}
\end{figure}


\subsection{TATAS Lock}

\begin{figure}\label{fig:step4_1}
\centering
\includegraphics[scale=0.65]{step4_1.png}
\caption{Speed of the mutex-locked and TaTaS shared arrays for $X=5000$}
\end{figure}


The TaTaS-lock performed slightly better than the mutex-lock for small numbers of threads due to its better cache line behaviour, but as the number increased, the performance of the TaTaS lock deteriorated and the standard deviation of its operation increased greatly (Fig \ref{fig:step4_1}).  This is potentially due to the overhead of having to do an extra test before test and setting, slightly delaying the operation of the code. Another potential culprit is the larger number of memory bus and cache transactions per operation.

\todo{better explanation}

\subsection{Reader-Writer Lock}
\begin{figure}\label{fig:step5_1}
\centering
\includegraphics[scale=0.65]{step5_1.png}
\caption{Speed of the mutex-locked array versus the reader-writer locked array when instantiated with various sizes}
\end{figure}

\begin{figure}\label{fig:step5_2}
\centering
\includegraphics[scale=0.65]{step5_2.png}
\caption{Speed of the mutex-locked array versus the reader-writer locked array when instantiated with various sizes, drawn without error bars to more clearly show the mean-behaviour.}
\end{figure}

When the array size is not negligibly small, the reader-writer lock significantly outperforms the standard mutex lock as it allows multiple summing operations to occur at once. It also has a much smaller standard deviation for each operation than seen in mutex case (Figs \ref{fig:step5_1}, \ref{fig:step5_2}). This may be due to the much shorter array access delay time due to not having to wait for a lock on the array in order to read the array. As a result, fewer coincidental pathological cases arose where the threads blocked on each other.

\subsection{Flag-Based Lock}

\begin{figure}\label{fig:step6_1}
\centering
\includegraphics[scale=0.65]{step6_1.png}
\caption{Speed of the mutex-locked array versus the flag-locked array when instantiated with various sizes}
\end{figure}

The flag based lock outperformed the mutex-lock less significantly than the reader-writer lock but with an even tighter standard deviation due to the cache lines containing flags not 

\begin{figure}\label{fig:step6_2}
\centering
\includegraphics[scale=0.65]{step6_2.png}
\caption{Speed of the mutex-locked array versus the flag-locked array when instantiated with various sizes, drawn without error bars to more clearly show the mean-behaviour.}
\end{figure}

\subsection{Write Mode}


For the write-mode experiments, I fixed the array size to be $X = 1000$. Further more, since all experiments up to this point had been run to make 100,000 calls to the \texttt{sum} function in the main thread before halting, running a writing\texttt{update} operation every 100k iterations would have resulted in barely any write operations. Hence I decided to deviate from the specification and run \texttt{update}s every 10,000 iterations in the sparse update case.
In the frequent update case, I kept $K=100$ as specified.
\begin{figure}\label{fig:step7_1}
\centering
\includegraphics[scale=0.65]{step7_1.png}
\caption{Performance of each type of lock when used for update operations every 10,000 operations.}
\end{figure}

\begin{figure}\label{fig:step7_2}
\centering
\includegraphics[scale=0.65]{step7_2.png}
\caption{Performance of each type of lock when used for update operations every 10,000 operations, drawn without error bars to more clearly show the mean-behaviour.}
\end{figure}


\todo{This is now wrong}
Other than some anomalous cases, all of the more advanced locks outperformed the naive mutex lock in the sparse update case (Figs \ref{fig:step7_1}, \ref{fig:step7_2}). The flags based lock appeared to be the best performer in the higher thread count cases, though the variance of results outweighed the difference in means.

\begin{figure}\label{fig:step7_3}
\centering
\includegraphics[scale=0.65]{step7_3.png}
\caption{Performance of each type of lock when used for update operations every 100 operations}
\end{figure}

\begin{figure}\label{fig:step7_4}
\centering
\includegraphics[scale=0.65]{step7_4.png}
\caption{Performance of each type of lock when used for update operations every 100 operations, drawn without error bars to more clearly show the mean-behaviour.}
\end{figure}

More interesting trends emerge when one looks at higher frequency write operations. In the $K=100$ case (Figs \ref{fig:step7_3}, \ref{fig:step7_4}), it can clearly be seen that the naive mutex case performs as poorly as the other worst other locks. However, the flags based locks and TaTaS locks performed well across all thread counts.

\section{Summary}

One issue with these experiments has been the non-reproducibility of the tests. Despite the high number of repeats (200 tests per batch) and length of tests in terms of operations (100k operations per test), and reviews of my experimental methods, I have seen poor reproducibility. Another anomaly is the general lack of performance degradation as the number of threads increased. In each test case, we would expect the amount of time to complete to increase by around a factor of two between 8 threads and 16, however this has not occurred. This suggests there is an additional overhead in the tests brought about by the operation of the JVM. Perhaps using a lower level language such as C or C++ with compiler optimisations turned off might cause trends to be more obvious.

\end{document}